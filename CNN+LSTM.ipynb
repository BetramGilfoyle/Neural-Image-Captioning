{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN+LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYVxWu8pz90_",
        "colab_type": "text"
      },
      "source": [
        "# CNN+LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pot5NMyBbjfe"
      },
      "source": [
        "## Setting up Kaggle API and downloading the dataset -Flickr30k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av7G0CTi0RBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD8H2mNB0c7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCjeeUwg1X2t",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck_NvvW01avH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download from Kaggle\n",
        "!kaggle datasets download -d hsankesara/flickr-image-dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sAzV6Bd1eEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip dataset\n",
        "!unzip flickr-image-dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qMY0CwC1iDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm flickr-image-dataset.zip\n",
        "!rm -r flickr30k_images/flickr30k_images/flickr30k_images\n",
        "!rm flickr30k_images/flickr30k_images/results.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g889F9r1r-_",
        "colab_type": "text"
      },
      "source": [
        "## Importing Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjR7InkT1t6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "!pip install torchtext==0.5.0\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer, Field\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jngJFM0d15Xr",
        "colab_type": "text"
      },
      "source": [
        "### Loading Flickr30k and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEkAq5R1znI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'flickr30k_images/flickr30k_images'\n",
        "csv_path = 'flickr30k_images/results.csv'\n",
        "!mkdir pretrained_models\n",
        "save_path = 'pretrained_models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URsambyQ2Gll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fixing a data issue\n",
        "results = pd.read_csv(csv_path, sep='|')\n",
        "fix_19999 = results.loc[19999][' comment_number']\n",
        "results.loc[19999][' comment_number'] = ' 4'\n",
        "results.loc[19999][' comment'] = fix_19999[4:]\n",
        "results = results.sort_values(by=[' comment_number', 'image_name', ])\n",
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofMigsOr2KeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flickr30kDataset(Dataset):\n",
        "    def __init__(self, data_path, transforms, comment_num):\n",
        "        self.data_path = data_path\n",
        "        self.data_files = os.listdir(self.data_path)\n",
        "        self.data_files = sorted(self.data_files)\n",
        "        self.transforms = transforms\n",
        "        assert 0 <= comment_num <= 4\n",
        "        self.comment_num = ' ' + str(comment_num)\n",
        "        self._tokenizer()\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_path, self.data_files[idx])\n",
        "        img = Image.open(img_path)\n",
        "        inputs = self.transforms(img)\n",
        "        captions = self.token_list[idx]\n",
        "        cap_lens = self.len_list[idx]\n",
        "        return inputs, captions, cap_lens\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_files)\n",
        "    \n",
        "    def _tokenizer(self):\n",
        "        self.caption_list = results.loc[results[' comment_number'] == self.comment_num][' comment'].tolist()\n",
        "        tokenizer = get_tokenizer(\"basic_english\")\n",
        "        self.token_list = [tokenizer(caption) for caption in self.caption_list]\n",
        "        self.len_list = torch.tensor([len(token) for token in self.token_list])\n",
        "        self.len_list += 1 # allow for <sos> or <eos>\n",
        "        self.seq_len = self.len_list.max() + 1\n",
        "        self.field = Field(tokenize='spacy', tokenizer_language='en', \n",
        "                           init_token='<sos>', eos_token='<eos>', lower=True, fix_length=self.seq_len)\n",
        "        self.field.build_vocab(self.token_list)\n",
        "        self.token_list = self.field.process(self.token_list)\n",
        "        self.token_list = self.token_list.transpose(1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmlGmm1m2Qrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms_train = transforms.Compose([\n",
        "        transforms.Resize(size=(224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r-N1m9u2Z-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_set = Flickr30kDataset(data_path, transforms_train, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikB1oLCc2eJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reference_corpus = []\n",
        "for num in range(5):\n",
        "    caption_candidates = results.loc[results[' comment_number'] == ' '+str(num)][' comment'].tolist()\n",
        "    print(caption_candidates[0])\n",
        "    reference_corpus.append([x.split() for x in caption_candidates])\n",
        "reference_corpus = np.array(reference_corpus)\n",
        "reference_corpus = reference_corpus.transpose(1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lb-TgK82gac",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBKawlaL2jz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_data = len(data_set)\n",
        "idx = list(range(num_data))\n",
        "train_set = Subset(data_set, idx[:-2000])\n",
        "vali_set = Subset(data_set, idx[-2000:-1000])\n",
        "test_set = Subset(data_set, idx[-1000:])\n",
        "print(len(train_set), len(vali_set), len(test_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlouMzZe2s9m",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWauWX7g2vmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, cnn):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.cnn = cnn\n",
        "        \n",
        "    def forward(self, x):\n",
        "        enc_output = self.cnn(x)\n",
        "        enc_output = F.relu(enc_output)\n",
        "        return enc_output\n",
        "\n",
        "    def freeze_bottom(self):\n",
        "        for p in self.cnn.parameters():\n",
        "            p.requires_grad = False\n",
        "        for c in list(self.cnn.children())[-3:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "    def freeze_all(self):\n",
        "        for p in self.cnn.parameters():\n",
        "            p.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD_vliNN3A0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, enc_size, emb_size, vocab_size, num_layers=1, embedding_matrix=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.enc_size = enc_size\n",
        "        self.hidden_size = enc_size\n",
        "        self.emb_size = emb_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.emb_size)\n",
        "        self.lstm = nn.LSTM(input_size=self.emb_size+self.enc_size, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "        \n",
        "    def forward(self, enc_out, captions, caplens):\n",
        "        enc_out = enc_out.unsqueeze(0) # 1 * batch_size * 1000\n",
        "        h0 = enc_out.cuda()\n",
        "        c0 = enc_out.clone().cuda()\n",
        "        captions = self.embed(captions) # seq_len * batch_size * emb_size\n",
        "        enc_out = enc_out.repeat(captions.size(0), 1, 1) # seq_len * batch_size * 1000\n",
        "        packed_captions = pack_padded_sequence(torch.cat((captions, enc_out), dim=2), caplens) # concatenate tokens and features\n",
        "        outputs, _ = self.lstm(packed_captions, (h0, c0))\n",
        "        outputs = self.fc(outputs[0])\n",
        "        return outputs\n",
        "\n",
        "    def greedy_pred(self, enc_out, pred_len):\n",
        "        init = torch.ones((1, enc_out.size(0)), dtype=int) *  2\n",
        "        init = init.cuda()\n",
        "        enc_out = enc_out.unsqueeze(0) \n",
        "        h0 = enc_out.cuda()\n",
        "        c0 = enc_out.clone().cuda()\n",
        "        init = self.embed(init) # 1 * batch_size * emb_size\n",
        "        next_out, (h, c) = self.lstm(torch.cat((init, enc_out), dim=2), (h0, c0))\n",
        "        next_out = self.fc(next_out)\n",
        "        next_in = next_out.argmax(dim=2)\n",
        "        outputs = next_in.clone()\n",
        "        for i in range(pred_len - 1):\n",
        "            next_in = self.embed(next_in)\n",
        "            next_out, (h, c) = self.lstm(torch.cat((next_in, enc_out), dim=2), (h, c))\n",
        "            next_out = self.fc(next_out)\n",
        "            next_in = next_out.argmax(dim=2)\n",
        "            outputs = torch.cat((outputs, next_in), dim=0)\n",
        "        return outputs\n",
        "\n",
        "    def beam_search_pred(self, enc_outs, pred_len, beam_size=3):\n",
        "        batch_size = enc_outs.size(0)\n",
        "        outputs = torch.ones((pred_len, batch_size), dtype=int) # place_holder for outputs\n",
        "        for idx in range(batch_size):\n",
        "            enc_out = enc_outs[idx]\n",
        "            enc_out = enc_out.unsqueeze(0).unsqueeze(0) # 1 * 1 * enc_size\n",
        "            enc_out = enc_out.repeat(1, beam_size, 1) # 1 * beam_size * enc_size, (view beam_size as batch_size for convenience)\n",
        "            k_words = torch.ones((1, beam_size), dtype=int).cuda() * 2 # 1 * beam_size\n",
        "            seqs = k_words # 1 * beam_size\n",
        "            k_scores = torch.zeros(1, beam_size, 1).cuda() # 1 * beam_size * 1\n",
        "            h = enc_out.cuda() # 1 * beam_size * enc_size\n",
        "            c = enc_out.clone().cuda() # 1 * beam_size * enc_size\n",
        "\n",
        "            for step in range(pred_len):\n",
        "                embedding = self.embed(k_words) # 1 * beam_size * emb_size\n",
        "                lstm_out, (h, c) = self.lstm(torch.cat((embedding, enc_out), dim=2), (h, c))\n",
        "                scores = self.fc(lstm_out) # 1 * beam_size * vocab_size\n",
        "                scores =  F.log_softmax(scores, dim=2) # 1 * beam_size * vocab_size\n",
        "                scores = k_scores.expand_as(scores) + scores # 1 * beam_size * vocab_size\n",
        "                if step == 0: # first step\n",
        "                    scores = scores.squeeze() # beam_size * vocab_size\n",
        "                    k_scores, k_words = scores[0].topk(beam_size) # beam_size\n",
        "                else:\n",
        "                    scores = scores.squeeze() # beam_size * vocab_size\n",
        "                    k_scores, k_words = scores.view(-1).topk(beam_size) # beam_size\n",
        "                prev_idx = k_words / self.vocab_size # beam_size (between 0 and beam_size)\n",
        "                next_idx = k_words % self.vocab_size # beam_size\n",
        "                seqs = seqs[:, prev_idx] # L * beam_size\n",
        "                seqs = torch.cat((seqs, next_idx.unsqueeze(0)), dim=0) # L * beam_size\n",
        "                k_scores = k_scores.unsqueeze(0).unsqueeze(-1)\n",
        "                k_words = next_idx.unsqueeze(0)\n",
        "            output = seqs[:, k_scores.squeeze().argmax()]\n",
        "            outputs[:, idx] = output[1:] # Don't include <sos>\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj-4eMVX3IuI",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjY9QPo-3KuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "batch_size = 32\n",
        "enc_lr = 1e-4\n",
        "dec_lr = 1e-4\n",
        "patience = 10\n",
        "enc_save_path, dec_save_path = os.path.join(save_path, 'best_enc_lstm_f30'+'_demo'), os.path.join(save_path, 'best_dec_lstm_f30'+'_demo')\n",
        "best_acc = 0\n",
        "best_epoch = 0\n",
        "\n",
        "enc_size = 1000\n",
        "emb_size = 300\n",
        "vocab_size = len(data_set.field.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhL7tFHA3UJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "vali_loader = DataLoader(vali_set, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O83OZPLc3XFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet = models.resnet50(pretrained=True)\n",
        "encoder = Encoder(resnet)\n",
        "encoder.freeze_all() # Freeze all or bottom\n",
        "decoder = Decoder(enc_size=enc_size, emb_size=emb_size, vocab_size=vocab_size)\n",
        "encoder, decoder = encoder.cuda(), decoder.cuda()\n",
        "del resnet\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTGA7oKC3k9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4zxIwiR3oyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_accuracy(pred, targets, ignore_index=data_set.field.vocab.stoi['<pad>']):\n",
        "    \"\"\"\n",
        "    pred: logit output\n",
        "    target: labels\n",
        "    ignore_index: exclude <pad> when calculating accuracy\n",
        "    \"\"\"\n",
        "    mask = ~targets.eq(ignore_index).cuda()\n",
        "    pred = pred[mask]\n",
        "    targets = targets[mask]\n",
        "    num_correct = pred.argmax(dim=1).eq(targets).sum()\n",
        "    acc = num_correct.float() / targets.size(0)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc6jGHxy3u7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=data_set.field.vocab.stoi['<pad>'])\n",
        "#enc_optimizer = torch.optim.AdamW([p for p in encoder.parameters() if p.requires_grad], lr=enc_lr)\n",
        "dec_optimizer = torch.optim.AdamW(decoder.parameters(), lr=dec_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34ck4xz035AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    train_loss = AverageMeter()\n",
        "    vali_loss = AverageMeter()\n",
        "    vali_acc = AverageMeter()\n",
        "    for batch_index, (inputs, captions, caplens) in enumerate(train_loader):\n",
        "        inputs, captions = inputs.cuda(), captions.cuda()\n",
        "        dec_optimizer.zero_grad()\n",
        "        enc_out = encoder(inputs)\n",
        "        captions = captions.transpose(0, 1)\n",
        "        caplens_sorted, sort_id = caplens.sort(descending=True)\n",
        "        captions_sorted = captions[:, sort_id]\n",
        "        captions_input = captions_sorted[:-1, :]\n",
        "        captions_target = captions_sorted[1:, :]\n",
        "        enc_out_sorted = enc_out[sort_id]\n",
        "        outputs = decoder(enc_out_sorted, captions_input, caplens_sorted)\n",
        "        captions_target_sorted = pack_padded_sequence(captions_target, caplens_sorted)\n",
        "        loss = criterion(outputs, captions_target_sorted[0])\n",
        "        loss.backward()\n",
        "        # enc_optimizer.step()\n",
        "        dec_optimizer.step()\n",
        "        train_loss.update(loss.item(), inputs.size(0))\n",
        "        if batch_index % 40 == 0:\n",
        "            print(\"Batch: {}, loss {:.4f}.\".format(batch_index, loss.item()))\n",
        "    # Evaluation\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_index, (inputs, captions, caplens) in enumerate(vali_loader):\n",
        "            inputs, captions = inputs.cuda(), captions.cuda()\n",
        "            enc_out = encoder(inputs)\n",
        "            captions = captions.transpose(0, 1)\n",
        "            caplens_sorted, sort_id = caplens.sort(descending=True)\n",
        "            captions_sorted = captions[:, sort_id]\n",
        "            captions_input = captions_sorted[:-1, :]\n",
        "            captions_target = captions_sorted[1:, :]\n",
        "            enc_out_sorted = enc_out[sort_id]\n",
        "            outputs = decoder(enc_out_sorted, captions_input, caplens_sorted)\n",
        "            captions_target_sorted = pack_padded_sequence(captions_target, caplens_sorted)\n",
        "            loss = criterion(outputs, captions_target_sorted[0])\n",
        "            acc = mask_accuracy(outputs, captions_target_sorted[0])\n",
        "            vali_loss.update(loss.item(), inputs.size(0))\n",
        "            vali_acc.update(acc, inputs.size(0))\n",
        "    print(\"Epoch: {}/{}, training loss: {:.4f}, vali loss: {:.4f}, vali acc: {:.4f}.\".format(epoch, epochs, train_loss.avg, vali_loss.avg, vali_acc.avg))\n",
        "    # Save best\n",
        "    if vali_acc.avg > best_acc:\n",
        "        best_acc = vali_acc.avg\n",
        "        best_epoch = epoch\n",
        "        torch.save(encoder.state_dict(), enc_save_path)\n",
        "        torch.save(decoder.state_dict(), dec_save_path)\n",
        "    # Early stopping\n",
        "    if epoch - best_epoch >= patience:\n",
        "        print(\"Early stopping\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpGj534F4aH0",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hte7y6yt4Um7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def token_sentence(decoder_out, itos):\n",
        "    tokens = decoder_out\n",
        "    # tokens = decoder_out.argmax(dim=2)\n",
        "    tokens = tokens.transpose(1, 0)\n",
        "    tokens = tokens.cpu().numpy()\n",
        "    results = []\n",
        "    for instance in tokens:\n",
        "        result = ' '.join([itos[x] for x in instance])\n",
        "        results.append(''.join(result.partition('<eos>')[0])) # Cut before '<eos>'\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqsQQqec4ht6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder.load_state_dict(torch.load(enc_save_path))\n",
        "decoder.load_state_dict(torch.load(dec_save_path))\n",
        "encoder.eval()\n",
        "decoder.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_SxAB4v4nc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "itos = data_set.field.vocab.itos\n",
        "pred_len = data_set.seq_len\n",
        "result_collection = []\n",
        "\n",
        "# Predictions with greedy\n",
        "with torch.no_grad():\n",
        "    for batch_index, (inputs, captions, caplens) in enumerate(test_loader):\n",
        "        inputs, captions = inputs.cuda(), captions.cuda()\n",
        "        enc_outs = encoder(inputs)\n",
        "        outputs = decoder.greedy_pred(enc_outs, pred_len)\n",
        "        result_caption = token_sentence(outputs, itos)\n",
        "        result_collection.extend(result_caption)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-47VTC0a4rRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "itos = data_set.field.vocab.itos\n",
        "pred_len = data_set.seq_len\n",
        "result_collection_bs = []\n",
        "\n",
        "# Predictions with beam search\n",
        "with torch.no_grad():\n",
        "    for batch_index, (inputs, captions, caplens) in enumerate(test_loader):\n",
        "        inputs, captions = inputs.cuda(), captions.cuda()\n",
        "        enc_outs = encoder(inputs)\n",
        "        outputs = decoder.beam_search_pred(enc_outs, pred_len, beam_size=3)\n",
        "        result_caption_bs = token_sentence(outputs, itos)\n",
        "        result_collection_bs.extend(result_caption_bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GQLY7ZV40Hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotax(ax, i):\n",
        "    ax.imshow(Image.open(os.path.join(data_path, data_set.data_files[-1000+i])))\n",
        "    ax.axis('off')\n",
        "    ax.set_title('\\n'.join(wrap(result_collection[i], 32)), fontsize=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjMqo0gT4_M1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textwrap import wrap\n",
        "fig, axs = plt.subplots(4, figsize=(3, 3))\n",
        "\n",
        "fig.subplots_adjust(top=3)\n",
        "\n",
        "plotax(axs[0], 10)\n",
        "plotax(axs[1], 20)\n",
        "plotax(axs[2], 50)\n",
        "plotax(axs[3], 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxh5E3VG5QMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualize\n",
        "i = 10\n",
        "plt.imshow(Image.open(os.path.join(data_path, data_set.data_files[-1000+i])))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(\"Ground truth:\", data_set.caption_list[-1000+i])\n",
        "print(\"Prediction-greedy:\", result_collection[i])\n",
        "print(\"Prediction-beam search:\", result_collection_bs[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzDyJZaD5aHL",
        "colab_type": "text"
      },
      "source": [
        "## BLEU Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P93V5Wnl5YSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bleu scores - Greedy search w.r.t. all candidates\n",
        "uni_bleu = bleu_score([x.split(' ') for x in result_collection], reference_corpus[-1000:], max_n=1, weights=[1])\n",
        "bi_bleu = bleu_score([x.split(' ') for x in result_collection], reference_corpus[-1000:], max_n=2, weights=[1/2]*2)\n",
        "tri_bleu = bleu_score([x.split(' ') for x in result_collection], reference_corpus[-1000:], max_n=3, weights=[1/3]*3)\n",
        "qua_bleu = bleu_score([x.split(' ') for x in result_collection], reference_corpus[-1000:], max_n=4, weights=[1/4]*4)\n",
        "uni_bleu, bi_bleu, tri_bleu, qua_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhgZuRwq5enY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bleu scores - Beam search w.r.t. all candidates\n",
        "uni_bleu = bleu_score([x.split(' ') for x in result_collection_bs], reference_corpus[-1000:], max_n=1, weights=[1])\n",
        "bi_bleu = bleu_score([x.split(' ') for x in result_collection_bs], reference_corpus[-1000:], max_n=2, weights=[1/2]*2)\n",
        "tri_bleu = bleu_score([x.split(' ') for x in result_collection_bs], reference_corpus[-1000:], max_n=3, weights=[1/3]*3)\n",
        "qua_bleu = bleu_score([x.split(' ') for x in result_collection_bs], reference_corpus[-1000:], max_n=4, weights=[1/4]*4)\n",
        "uni_bleu, bi_bleu, tri_bleu, qua_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}